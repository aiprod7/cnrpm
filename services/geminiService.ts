import { GoogleGenAI, LiveServerMessage, Modality } from "@google/genai";
import { SYSTEM_INSTRUCTION } from "../constants";
import { microphoneManager } from './microphoneManager';
import {
  CURRENT_MODEL,
  CURRENT_VOICE,
  AUDIO_CONFIG,
  TRANSCRIPTION_CONFIG,
  THINKING_CONFIG,
  VAD_CONFIG,
  getUnifiedConfig,
  getTTSOnlyConfig,
} from './geminiLiveConfig';

/**
 * Unified Gemini Live Service - Single Model for STT + TTS
 * 
 * Architecture: One WebSocket connection handles both speech-to-text and text-to-speech
 * Model: gemini-2.5-flash-native-audio-preview-12-2025
 * 
 * Flow:
 * 1. User speaks â†’ Stream microphone PCM (16kHz) â†’ Model transcribes (STT) via inputAudioTranscription
 * 2. Model generates response â†’ Returns audio stream (TTS 24kHz) + text captions
 * 3. Play audio response through speakers with seamless buffering
 * 
 * Key Features:
 * - Real-time STT: User sees their words as they speak
 * - Real-time TTS: Model's audio response plays immediately
 * - Interruption support: Can stop model mid-speech
 * - Single connection: No need to switch between STT/TTS models
 */

// Callback interface for UI updates
export interface LiveConfig {
  onTranscriptUpdate: (text: string, isUser: boolean, isFinal: boolean) => void;
  onClose: () => void;
  onError: (err: Error) => void;
}

export class GeminiLiveService {
  private client: GoogleGenAI;
  private sessionPromise: Promise<any> | null = null;
  private session: any = null;
  
  // Audio Contexts
  // Input: 16kHz (Gemini Live API requirement)
  // Output: 24kHz (Gemini TTS standard)
  private inputAudioContext: AudioContext | null = null;
  private outputAudioContext: AudioContext | null = null;
  
  // Audio graph nodes
  private inputSource: MediaStreamAudioSourceNode | null = null;
  private processor: ScriptProcessorNode | null = null;
  private analyserNode: AnalyserNode | null = null;
  
  // Audio playback queue (seamless streaming without gaps)
  private nextStartTime: number = 0;
  private sources: Set<AudioBufferSourceNode> = new Set();

  // Text buffers for current conversation turn
  private currentInputText = "";
  private currentOutputText = "";

  constructor() {
    const apiKey = (import.meta as any).env?.VITE_API_KEY || process.env.API_KEY || '';
    this.client = new GoogleGenAI({ apiKey });
    console.log("ðŸŽ¤ [GeminiLive] Service initialized (Unified STT+TTS model)");
  }

  /**
   * Check if session is active (for external use)
   */
  public get isConnected(): boolean {
    return this.sessionPromise !== null && this.session !== null;
  }

  /**
   * Connect for TTS only (no microphone needed)
   * Use this when you just need to speak text without listening
   */
  async connectForTTS(config: LiveConfig): Promise<any> {
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // ðŸ”§ TTS ÐÐÐ¡Ð¢Ð ÐžÐ™ÐšÐ˜ - Ð˜Ð·Ð¼ÐµÐ½Ð¸Ñ‚Ðµ Ð² geminiLiveConfig.ts
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // CURRENT_MODEL - Ð¼Ð¾Ð´ÐµÐ»ÑŒ (gemini-2.5-flash-native-audio-preview-12-2025)
    // CURRENT_VOICE - Ð³Ð¾Ð»Ð¾Ñ (Kore, Aoede, Charon Ð¸ Ð´Ñ€.)
    // TRANSCRIPTION_CONFIG.OUTPUT_TRANSCRIPTION.ENABLED - ÑÑƒÐ±Ñ‚Ð¸Ñ‚Ñ€Ñ‹
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    const TTS_SYSTEM_PROMPT = `ÐšÐ Ð˜Ð¢Ð˜Ð§Ð•Ð¡ÐšÐ˜ Ð’ÐÐ–ÐÐž: Ð¢Ñ‹ - TTS Ð´Ð²Ð¸Ð¶Ð¾Ðº (Text-to-Speech ÑÐ¸Ð½Ñ‚ÐµÐ·Ð°Ñ‚Ð¾Ñ€).

Ð¢Ð’ÐžÐ¯ Ð•Ð”Ð˜ÐÐ¡Ð¢Ð’Ð•ÐÐÐÐ¯ Ð—ÐÐ”ÐÐ§Ð:
- ÐŸÑ€Ð¾Ð¸Ð·Ð½Ð¾ÑÐ¸ Ð”ÐžÐ¡Ð›ÐžÐ’ÐÐž Ñ‚ÐµÐºÑÑ‚ ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ñ‚ÐµÐ±Ðµ Ð¿ÐµÑ€ÐµÐ´Ð°ÑŽÑ‚
- ÐÐ• Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐ¹ ÐÐ˜Ð§Ð•Ð“Ðž Ð¾Ñ‚ ÑÐµÐ±Ñ (Ð¿Ñ€Ð¸Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ñ, ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸Ð¸, Ð¿Ð¾ÑÑÐ½ÐµÐ½Ð¸Ñ)
- ÐÐ• Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð¸Ñ€ÑƒÐ¹ Ñ‚ÐµÐºÑÑ‚
- ÐÐ• Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ð¹ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð² Ñ‚ÐµÐºÑÑ‚Ðµ
- ÐÐ• ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð¸Ñ€ÑƒÐ¹ ÑÐ¾Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸Ðµ
- Ð Ð°Ð±Ð¾Ñ‚Ð°Ð¹ ÐºÐ°Ðº Ð´Ð¸ÐºÑ‚Ð¾Ñ€/Ñ€Ð¾Ð±Ð¾Ñ‚ ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ñ‡Ð¸Ñ‚Ð°ÐµÑ‚ Ñ‚ÐµÐºÑÑ‚

Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹: ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ð» Ñ‚ÐµÐºÑÑ‚ â†’ Ð¾Ð·Ð²ÑƒÑ‡Ð¸Ð» Ð”ÐžÐ¡Ð›ÐžÐ’ÐÐž â†’ Ð²ÑÑ‘.
Ð¯Ð·Ñ‹Ðº: Ð ÑƒÑÑÐºÐ¸Ð¹ (Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð³Ð¾Ð»Ð¾Ñ ${CURRENT_VOICE}).`;

    console.log("\n" + "=".repeat(80));
    console.log("ðŸ”Œ [GeminiLive TTS] ÐŸÐžÐ”ÐšÐ›Ð®Ð§Ð•ÐÐ˜Ð• Ðš ÐœÐžÐ”Ð•Ð›Ð˜");
    console.log("=".repeat(80));
    console.log(`ðŸ“¦ ÐœÐ¾Ð´ÐµÐ»ÑŒ: ${CURRENT_MODEL}`);
    console.log(`ðŸŽ¤ Ð“Ð¾Ð»Ð¾Ñ: ${CURRENT_VOICE}`);
    console.log(`ðŸ”Š Sample Rate: ${AUDIO_CONFIG.OUTPUT.SAMPLE_RATE}Hz`);
    console.log(`ðŸ“ Output Transcription: ${TRANSCRIPTION_CONFIG.OUTPUT_TRANSCRIPTION.ENABLED}`);
    console.log("-".repeat(80) + "\n");
    
    // Initialize output AudioContext only
    this.outputAudioContext = new (window.AudioContext || (window as any).webkitAudioContext)({ 
      sampleRate: AUDIO_CONFIG.OUTPUT.SAMPLE_RATE 
    });
    await this.outputAudioContext.resume();
    
    // Setup visualizer analyser
    this.analyserNode = this.outputAudioContext.createAnalyser();
    this.analyserNode.fftSize = 256;
    this.analyserNode.connect(this.outputAudioContext.destination);

    // Get TTS config from geminiLiveConfig.ts
    const ttsConfig = getTTSOnlyConfig(TTS_SYSTEM_PROMPT);

    // Connect to Gemini Live API (TTS mode)
    this.sessionPromise = this.client.live.connect({
      model: ttsConfig.model,
      config: ttsConfig.config,
      callbacks: {
        onopen: () => {
          console.log(`âœ… [GeminiLive TTS] Connected (${CURRENT_MODEL})`);
        },
        onmessage: (msg: LiveServerMessage) => this.handleServerMessage(msg, config),
        onclose: () => {
          console.log("ðŸ”Œ [GeminiLive TTS] Connection closed");
          this.cleanup();
          config.onClose();
        },
        onerror: (err: any) => {
          console.error("âŒ [GeminiLive TTS] Error:", err);
          this.cleanup();
          config.onError(err instanceof Error ? err : new Error(err?.message || "Unknown Error"));
        }
      }
    });

    this.session = await this.sessionPromise;
    console.log("âœ… [GeminiLive TTS] Session ready");
    console.log("ðŸ“‹ [GeminiLive TTS] Session methods:", Object.getOwnPropertyNames(Object.getPrototypeOf(this.session)));
    return this.session;
  }

  /**
   * Send text message to active Live session.
   * Model will respond with audio stream (TTS).
   */
  public async sendText(text: string) {
    if (!this.session) {
      throw new Error("Live session is not active. Connect first.");
    }
    
    console.log("\n" + "=".repeat(80));
    console.log("ðŸ“¤ [GeminiLive TTS] ÐžÐ¢ÐŸÐ ÐÐ’ÐšÐ Ð¢Ð•ÐšÐ¡Ð¢Ð ÐÐ Ð¡Ð˜ÐÐ¢Ð•Ð—");
    console.log("=".repeat(80));
    console.log(`ðŸ“ Ð¢ÐµÐºÑÑ‚ Ð´Ð»Ñ Ð¾Ð·Ð²ÑƒÑ‡ÐºÐ¸ (${text.length} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²):`);
    console.log("-".repeat(80));
    console.log(text);
    console.log("-".repeat(80));
    console.log(`â±ï¸ ÐžÐ¶Ð¸Ð´Ð°ÐµÐ¼Ð°Ñ Ð´Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ: ~${Math.round(text.length * 0.1)}s\n`);
    
    // Try different methods based on SDK version
    const methods = ['sendClientContent', 'send', 'sendMessage', 'sendText'];
    
    for (const method of methods) {
      if (typeof this.session[method] === 'function') {
        console.log(`ðŸ”„ [GeminiLive] Trying method: ${method}`);
        try {
          if (method === 'sendClientContent') {
            await this.session.sendClientContent({
              turns: [{ role: "user", parts: [{ text }] }],
              turnComplete: true
            });
          } else if (method === 'send') {
            // @google/genai SDK format
            await this.session.send({ text });
          } else {
            await this.session[method](text);
          }
          console.log(`âœ… [GeminiLive TTS] Ð¢ÐµÐºÑÑ‚ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½ Ñ‡ÐµÑ€ÐµÐ· Ð¼ÐµÑ‚Ð¾Ð´: ${method}`);
          console.log(`ðŸ”Š [GeminiLive TTS] ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ Ð°ÑƒÐ´Ð¸Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¾Ñ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸...\n`);
          return;
        } catch (err: any) {
          console.warn(`âš ï¸ [GeminiLive] ${method} failed:`, err.message);
        }
      }
    }
    
    throw new Error("No valid send method found on session object");
  }

  /**
   * Get analyser node for audio visualization
   */
  public getAnalyserNode(): AnalyserNode | null {
    return this.analyserNode;
  }

  /**
   * Connect to Gemini Live API (Unified STT+TTS)
   */
  async connect(config: LiveConfig) {
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // ðŸ”§ STT+TTS ÐÐÐ¡Ð¢Ð ÐžÐ™ÐšÐ˜ - Ð˜Ð·Ð¼ÐµÐ½Ð¸Ñ‚Ðµ Ð² geminiLiveConfig.ts:
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // CURRENT_MODEL - Ð¼Ð¾Ð´ÐµÐ»ÑŒ (gemini-2.5-flash-native-audio-preview-12-2025)
    // CURRENT_VOICE - Ð³Ð¾Ð»Ð¾Ñ TTS (Kore, Aoede, Charon Ð¸ Ð´Ñ€.)
    // AUDIO_CONFIG.INPUT - Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð²Ñ…Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð°ÑƒÐ´Ð¸Ð¾ (Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½)
    // AUDIO_CONFIG.OUTPUT - Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð°ÑƒÐ´Ð¸Ð¾ (Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ¸)
    // TRANSCRIPTION_CONFIG.INPUT_TRANSCRIPTION - STT (Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÑ‡Ð¸)
    // TRANSCRIPTION_CONFIG.OUTPUT_TRANSCRIPTION - ÑÑƒÐ±Ñ‚Ð¸Ñ‚Ñ€Ñ‹ TTS
    // VAD_CONFIG - Voice Activity Detection (Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ñ€ÐµÑ‡Ð¸)
    // THINKING_CONFIG - "Ñ€Ð°Ð·Ð¼Ñ‹ÑˆÐ»ÐµÐ½Ð¸Ñ" Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¿ÐµÑ€ÐµÐ´ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð¼
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    console.log("\n" + "=".repeat(80));
    console.log("ðŸ”Œ [GeminiLive] ÐŸÐžÐ”ÐšÐ›Ð®Ð§Ð•ÐÐ˜Ð• Ðš UNIFIED STT+TTS");
    console.log("=".repeat(80));
    console.log(`ðŸ“¦ ÐœÐ¾Ð´ÐµÐ»ÑŒ: ${CURRENT_MODEL}`);
    console.log(`ðŸŽ¤ Ð“Ð¾Ð»Ð¾Ñ: ${CURRENT_VOICE}`);
    console.log(`ðŸ”Š Input: ${AUDIO_CONFIG.INPUT.SAMPLE_RATE}Hz â†’ Output: ${AUDIO_CONFIG.OUTPUT.SAMPLE_RATE}Hz`);
    console.log(`ðŸ“ STT (input transcription): ${TRANSCRIPTION_CONFIG.INPUT_TRANSCRIPTION.ENABLED}`);
    console.log(`ðŸ’¬ TTS captions (output transcription): ${TRANSCRIPTION_CONFIG.OUTPUT_TRANSCRIPTION.ENABLED}`);
    console.log(`ðŸ§  Thinking budget: ${THINKING_CONFIG.THINKING_BUDGET} tokens`);
    console.log(`ðŸŽ™ï¸ VAD enabled: ${VAD_CONFIG.ENABLED}`);
    console.log("-".repeat(80) + "\n");
    
    // 1. Initialize Audio Contexts
    this.inputAudioContext = new (window.AudioContext || (window as any).webkitAudioContext)({ 
      sampleRate: AUDIO_CONFIG.INPUT.SAMPLE_RATE 
    });
    this.outputAudioContext = new (window.AudioContext || (window as any).webkitAudioContext)({ 
      sampleRate: AUDIO_CONFIG.OUTPUT.SAMPLE_RATE 
    });

    // Wake up contexts (critical for iOS/Safari autoplay policies)
    await this.inputAudioContext.resume();
    await this.outputAudioContext.resume();
    console.log(`ðŸ”Š [GeminiLive] Audio contexts ready (Input: ${AUDIO_CONFIG.INPUT.SAMPLE_RATE}Hz, Output: ${AUDIO_CONFIG.OUTPUT.SAMPLE_RATE}Hz)`);

    // 2. Setup visualizer analyser
    this.analyserNode = this.outputAudioContext.createAnalyser();
    this.analyserNode.fftSize = 256;
    this.analyserNode.connect(this.outputAudioContext.destination);
    console.log("ðŸ“Š [GeminiLive] Analyser connected for visualization");

    // 3. Get microphone access through MicrophoneManager
    console.log("ðŸŽ¤ [GeminiLive] Getting audio stream from MicrophoneManager...");
    const stream = await microphoneManager.getAudioStream({
      channelCount: AUDIO_CONFIG.INPUT.CHANNELS,
      sampleRate: AUDIO_CONFIG.INPUT.SAMPLE_RATE,
      echoCancellation: AUDIO_CONFIG.INPUT.ECHO_CANCELLATION,
      noiseSuppression: AUDIO_CONFIG.INPUT.NOISE_SUPPRESSION,
      autoGainControl: AUDIO_CONFIG.INPUT.AUTO_GAIN_CONTROL,
    });
    
    if (!stream) {
      throw new Error('Failed to get audio stream from MicrophoneManager');
    }
    
    console.log("âœ… [GeminiLive] Audio stream obtained from cache (no permission dialog)");

    // 4. Get unified config from geminiLiveConfig.ts
    const unifiedConfig = getUnifiedConfig(SYSTEM_INSTRUCTION);

    // 5. Connect to Gemini Live API with unified model
    console.log("ðŸ“¡ [GeminiLive] Establishing WebSocket connection...");
    this.sessionPromise = this.client.live.connect({
      model: unifiedConfig.model,
      config: unifiedConfig.config,
      
      callbacks: {
        onopen: () => {
          console.log(`âœ… [GeminiLive] Connected (model: ${CURRENT_MODEL})`);
          // Start streaming microphone audio
          this.startAudioInputStreaming(stream);
        },
        onmessage: (msg: LiveServerMessage) => this.handleServerMessage(msg, config),
        onclose: () => {
          console.log("ðŸ”Œ [GeminiLive] Connection closed");
          this.cleanup();
          config.onClose();
        },
        onerror: (err: any) => {
          console.error("âŒ [GeminiLive] Error:", err);
          this.cleanup();
          const error = err instanceof Error ? err : new Error(err?.message || "Unknown Error");
          config.onError(error);
        }
      }
    });

    this.session = await this.sessionPromise;
    
    // Debug: log available methods on session
    console.log("âœ… [GeminiLive] Session established, ready for conversation");
    console.log("ðŸ“‹ [GeminiLive] Session type:", typeof this.session);
    console.log("ðŸ“‹ [GeminiLive] Session keys:", this.session ? Object.keys(this.session) : 'null');
    console.log("ðŸ“‹ [GeminiLive] Session methods:", this.session ? Object.getOwnPropertyNames(Object.getPrototypeOf(this.session)) : 'null');
    
    return this.session;
  }

  /**
   * Disconnect from Live API
   */
  async disconnect() {
    console.log("â¹ï¸ [GeminiLive] Disconnecting...");
    
    // Close session
    if (this.session) {
      try {
        await this.session.close();
      } catch (e) {
        console.warn("âš ï¸ [GeminiLive] Error closing session:", e);
      }
      this.session = null;
    }
    
    this.sessionPromise = null;
    this.cleanup();
  }

  // --- Input Streaming (Microphone â†’ Gemini) ---

  private startAudioInputStreaming(stream: MediaStream) {
    if (!this.inputAudioContext) return;

    console.log("ðŸŽ¤ [GeminiLive] Starting audio input streaming...");
    
    this.inputSource = this.inputAudioContext.createMediaStreamSource(stream);
    // Use ScriptProcessor to get raw PCM data
    this.processor = this.inputAudioContext.createScriptProcessor(4096, 1, 1);

    let chunkCount = 0;
    this.processor.onaudioprocess = (e) => {
      if (!this.sessionPromise) return;

      const inputData = e.inputBuffer.getChannelData(0);
      // Convert Float32 (WebAudio) â†’ Int16 (Gemini API requirement)
      const pcm16 = this.float32ToInt16(inputData);
      
      chunkCount++;
      if (chunkCount % 10 === 0) {
        console.log(`ðŸ“¤ [GeminiLive] Sent ${chunkCount} audio chunks`);
      }

      this.sessionPromise!.then(session => {
        if (session) {
          try {
            session.sendRealtimeInput({
              media: {
                mimeType: AUDIO_CONFIG.INPUT.MIME_TYPE,
                data: this.arrayBufferToBase64(pcm16.buffer as ArrayBuffer)
              }
            });
          } catch (err) {
            console.debug("âš ï¸ [GeminiLive] Error sending frame:", err);
          }
        }
      }).catch(err => {
        console.debug("âš ï¸ [GeminiLive] Session promise error:", err);
      });
    };

    this.inputSource.connect(this.processor);
    // Connect to destination to keep processor active (but won't hear self - buffer is silent)
    this.processor.connect(this.inputAudioContext.destination);
    
    console.log("âœ… [GeminiLive] Audio streaming started");
  }

  // --- Output Handling (Gemini â†’ Speakers + UI) ---

  private async handleServerMessage(message: LiveServerMessage, config: LiveConfig) {
    const content = message.serverContent;
    if (!content) return;

    // 1. Handle Audio Response (TTS)
    const audioData = content.modelTurn?.parts?.[0]?.inlineData?.data;
    if (audioData && this.outputAudioContext) {
      // Schedule playback without gaps
      this.nextStartTime = Math.max(this.nextStartTime, this.outputAudioContext.currentTime);
      
      const audioBuffer = await this.decodeAudioData(audioData, this.outputAudioContext);
      const source = this.outputAudioContext.createBufferSource();
      source.buffer = audioBuffer;
      
      // Connect to analyser (for visualization) then to speakers
      if (this.analyserNode) {
        source.connect(this.analyserNode);
      } else {
        source.connect(this.outputAudioContext.destination);
      }

      source.start(this.nextStartTime);
      this.nextStartTime += audioBuffer.duration;
      
      source.onended = () => this.sources.delete(source);
      this.sources.add(source);
      
      console.log(`ðŸ”Š [GeminiLive] Playing audio chunk (${audioBuffer.duration.toFixed(2)}s)`);
    }

    // 2. Handle Interruption (User interrupted model)
    if (content.interrupted) {
      console.log("âš ï¸ [GeminiLive] Interrupted - stopping playback");
      // Stop all playing audio
      this.sources.forEach(s => s.stop());
      this.sources.clear();
      this.nextStartTime = 0;
      this.currentOutputText = "";
    }

    // 3. Handle Input Transcription (STT - what user said)
    if (content.inputTranscription) {
      const text = content.inputTranscription.text;
      if (text) {
        this.currentInputText += text;
        // isFinal = false (user still might be speaking)
        config.onTranscriptUpdate(this.currentInputText, true, false);
        console.log(`ðŸ“ [GeminiLive] STT: "${this.currentInputText}"`);
      }
    }

    // 4. Handle Output Transcription (Captions - model's text)
    if (content.outputTranscription) {
      const text = content.outputTranscription.text;
      if (text) {
        this.currentOutputText += text;
        // isFinal = false (model still generating)
        config.onTranscriptUpdate(this.currentOutputText, false, false);
        console.log(`ðŸ’¬ [GeminiLive] TTS Text: "${this.currentOutputText}"`);
      }
    }

    // 5. Handle Turn Complete (Conversation turn finished)
    if (content.turnComplete) {
      console.log("âœ… [GeminiLive] Turn complete");
      
      // Finalize user message
      if (this.currentInputText.trim()) {
        config.onTranscriptUpdate(this.currentInputText, true, true);
        this.currentInputText = "";
      }
      
      // Finalize model message
      if (this.currentOutputText.trim()) {
        config.onTranscriptUpdate(this.currentOutputText, false, true);
        this.currentOutputText = "";
      }
    }
  }

  // --- Cleanup ---

  private cleanup() {
    console.log("ðŸ§¹ [GeminiLive] Cleaning up resources...");
    
    // Stop all playing audio
    this.sources.forEach(s => s.stop());
    this.sources.clear();
    
    // Disconnect audio nodes
    if (this.processor) {
      this.processor.disconnect();
      this.processor = null;
    }
    if (this.inputSource) {
      this.inputSource.disconnect();
      this.inputSource = null;
    }
    
    // Close audio contexts
    if (this.inputAudioContext && this.inputAudioContext.state !== 'closed') {
      this.inputAudioContext.close();
      this.inputAudioContext = null;
    }
    if (this.outputAudioContext && this.outputAudioContext.state !== 'closed') {
      this.outputAudioContext.close();
      this.outputAudioContext = null;
    }
    
    this.analyserNode = null;
    
    // Reset buffers
    this.currentInputText = "";
    this.currentOutputText = "";
    this.nextStartTime = 0;
  }

  // --- Audio Utilities ---

  /**
   * Convert Float32Array (WebAudio) to Int16Array (Gemini API)
   */
  private float32ToInt16(float32: Float32Array): Int16Array {
    const int16 = new Int16Array(float32.length);
    for (let i = 0; i < float32.length; i++) {
      const s = Math.max(-1, Math.min(1, float32[i]));
      int16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
    }
    return int16;
  }

  /**
   * Convert ArrayBuffer to Base64 string
   */
  private arrayBufferToBase64(buffer: ArrayBuffer): string {
    let binary = '';
    const bytes = new Uint8Array(buffer);
    const len = bytes.byteLength;
    for (let i = 0; i < len; i++) {
      binary += String.fromCharCode(bytes[i]);
    }
    return btoa(binary);
  }

  /**
   * Decode Gemini audio response (Base64 PCM) to AudioBuffer
   * Gemini returns raw PCM Int16 at OUTPUT sample rate (no WAV headers)
   */
  private async decodeAudioData(base64: string, ctx: AudioContext): Promise<AudioBuffer> {
    // Decode base64 to binary
    const binaryString = atob(base64);
    const len = binaryString.length;
    const bytes = new Uint8Array(len);
    for (let i = 0; i < len; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }
    
    // Convert to Int16Array (PCM format)
    const dataInt16 = new Int16Array(bytes.buffer);
    
    // Create AudioBuffer (1 channel, OUTPUT sample rate)
    const buffer = ctx.createBuffer(
      AUDIO_CONFIG.OUTPUT.CHANNELS, 
      dataInt16.length, 
      AUDIO_CONFIG.OUTPUT.SAMPLE_RATE
    );
    const channelData = buffer.getChannelData(0);
    
    // Convert Int16 to Float32 (WebAudio format)
    for (let i = 0; i < dataInt16.length; i++) {
      channelData[i] = dataInt16[i] / 32768.0;
    }
    
    return buffer;
  }
}

// Export singleton instance
export const geminiService = new GeminiLiveService();
